C:\proyectos\proyectos09\SKYNNET\SW2\pruebas_masivas>py MLP_caltech_adam_split02.py 1
**************************************
*     MASSIVE MLP                    *
*   programa de test                 *
*                                    *
* usage:                             *
* py massive_MLP.py <RF>             *
*    RF: reduction factor to N/RF    *
* input params:                      *
*    - RF 1
*                                    *
**************************************
2024-02-02 09:50:29.680573: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
2024-02-02 09:50:29.681726: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2024-02-02 09:50:29.685556: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
informative features
--------------------
  class names: ['accordion', 'airplanes', 'anchor', 'ant', 'background_google', 'barrel', 'bass', 'beaver', 'binocular', 'bonsai', 'brain', 'brontosaurus', 'buddha', 'butterfly', 'camera', 'cannon', 'car_side', 'ceiling_fan', 'cellphone', 'chair', 'chandelier', 'cougar_body', 'cougar_face', 'crab', 'crayfish', 'crocodile', 'crocodile_head', 'cup', 'dalmatian', 'dollar_bill', 'dolphin', 'dragonfly', 'electric_guitar', 'elephant', 'emu', 'euphonium', 'ewer', 'faces', 'faces_easy', 'ferry', 'flamingo', 'flamingo_head', 'garfield', 'gerenuk', 'gramophone', 'grand_piano', 'hawksbill', 'headphone', 'hedgehog', 'helicopter', 'ibis', 'inline_skate', 'joshua_tree', 'kangaroo', 'ketch', 'lamp', 'laptop', 'leopards', 'llama', 'lobster', 'lotus', 'mandolin', 'mayfly', 'menorah', 'metronome', 'minaret', 'motorbikes', 'nautilus', 'octopus', 'okapi', 'pagoda', 'panda', 'pigeon', 'pizza', 'platypus', 'pyramid', 'revolver', 'rhino', 'rooster', 'saxophone', 'schooner', 'scissors', 'scorpion', 'sea_horse', 'snoopy', 'soccer_ball', 'stapler', 'starfish', 'stegosaurus', 'stop_sign', 'strawberry', 'sunflower', 'tick', 'trilobite', 'umbrella', 'watch', 'water_lilly', 'wheelchair', 'wild_cat', 'windsor_chair', 'wrench', 'yin_yang']
  num clases: 102

datasets sizes. IF batch_size is used THEN this is the number of batches
-------------------------------------------------------------------------
  Train set size:  9144  batches of  1  elements

ds_train contents ( caltech101 )
--------------------------------
<PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>

dataset to numpy conversion ( caltech101 )
------------------------------------------
  convirtiendo DS en arrays numpy...( caltech101 )
C:\proyectos\proyectos09\SKYNNET\SW2\pruebas_masivas\MLP_caltech_adam_split02.py:255: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train = np.array(list(map(lambda x: x[0], ds_train_npy)))
  conversion OK
   x_train numpy shape: (9144,)
   y_train numpy shape: (9144,)


deconstruccion
----------------
 n_clases:  102    reduction to  102
 NUM MAQUINAS= 1

Filter data (deconstruction)
---------------------------
  number of examples:
   x_train: (9144,)
   y_train: (9144,)

Aadaptando imagenes...( caltech101 )
------------------------------------
ds_name= caltech101 --> h2= 128 w2= 128

channels:  3
 nuevo array: samples: 9144  h2= 128  w2= 128  channels2= 3

Normalizando... caltech101
-------------------------
array A --> dtype= float32
array train --> dtype= uint8
datos cargados en arrays
OPTIMIZER= adam
---------------------------
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 flatten (Flatten)           (None, 49152)             0

 dense (Dense)               (None, 816)               40108848

 dense_1 (Dense)             (None, 408)               333336

 dense_2 (Dense)             (None, 204)               83436

 dense_3 (Dense)             (None, 102)               20910

=================================================================
Total params: 40,546,530
Trainable params: 40,546,530
Non-trainable params: 0
_________________________________________________________________
None

DS: caltech101  n_clases:  102 factor= 1  reduction to  102  categories using  1  machines

epocas finales: 30 , split = 0.2 batch= 32

Epoch 1/30
229/229 [==============================] - 58s 246ms/step - loss: 5.0875 - accuracy: 0.2078 - val_loss: 3.0664 - val_accuracy: 0.3783
Epoch 2/30
229/229 [==============================] - 52s 227ms/step - loss: 3.4540 - accuracy: 0.2714 - val_loss: 2.6988 - val_accuracy: 0.4226
Epoch 3/30
229/229 [==============================] - 51s 222ms/step - loss: 3.2267 - accuracy: 0.2951 - val_loss: 2.4584 - val_accuracy: 0.4489
Epoch 4/30
229/229 [==============================] - 51s 224ms/step - loss: 3.0622 - accuracy: 0.3167 - val_loss: 2.3403 - val_accuracy: 0.4691
Epoch 5/30
229/229 [==============================] - 52s 228ms/step - loss: 2.9361 - accuracy: 0.3300 - val_loss: 2.4663 - val_accuracy: 0.4423
Epoch 6/30
229/229 [==============================] - 52s 225ms/step - loss: 2.8579 - accuracy: 0.3414 - val_loss: 2.3256 - val_accuracy: 0.4784
Epoch 7/30
229/229 [==============================] - 52s 227ms/step - loss: 2.7302 - accuracy: 0.3583 - val_loss: 2.2905 - val_accuracy: 0.4833
Epoch 8/30
229/229 [==============================] - 54s 234ms/step - loss: 2.6682 - accuracy: 0.3692 - val_loss: 2.2359 - val_accuracy: 0.4997
Epoch 9/30
229/229 [==============================] - 51s 224ms/step - loss: 2.6002 - accuracy: 0.3784 - val_loss: 2.2096 - val_accuracy: 0.4975
Epoch 10/30
229/229 [==============================] - 51s 224ms/step - loss: 2.4846 - accuracy: 0.4021 - val_loss: 2.1488 - val_accuracy: 0.4975
Epoch 11/30
229/229 [==============================] - 56s 243ms/step - loss: 2.4312 - accuracy: 0.4108 - val_loss: 2.2026 - val_accuracy: 0.5112
Epoch 12/30
229/229 [==============================] - 50s 218ms/step - loss: 2.3790 - accuracy: 0.4260 - val_loss: 2.2409 - val_accuracy: 0.4926
Epoch 13/30
229/229 [==============================] - 51s 222ms/step - loss: 2.3119 - accuracy: 0.4279 - val_loss: 2.1506 - val_accuracy: 0.5243
Epoch 14/30
229/229 [==============================] - 50s 217ms/step - loss: 2.2245 - accuracy: 0.4528 - val_loss: 2.2225 - val_accuracy: 0.5238
Epoch 15/30
229/229 [==============================] - 50s 217ms/step - loss: 2.1657 - accuracy: 0.4571 - val_loss: 2.2626 - val_accuracy: 0.5085
Epoch 16/30
229/229 [==============================] - 50s 217ms/step - loss: 2.0757 - accuracy: 0.4752 - val_loss: 2.2454 - val_accuracy: 0.5150
Epoch 17/30
229/229 [==============================] - 50s 218ms/step - loss: 2.0495 - accuracy: 0.4867 - val_loss: 2.1547 - val_accuracy: 0.5243
Epoch 18/30
229/229 [==============================] - 50s 216ms/step - loss: 1.9635 - accuracy: 0.5059 - val_loss: 2.3066 - val_accuracy: 0.5041
Epoch 19/30
229/229 [==============================] - 49s 216ms/step - loss: 1.9204 - accuracy: 0.5117 - val_loss: 2.4094 - val_accuracy: 0.5150
Epoch 20/30
229/229 [==============================] - 50s 217ms/step - loss: 1.8627 - accuracy: 0.5219 - val_loss: 2.2526 - val_accuracy: 0.5336
Epoch 21/30
229/229 [==============================] - 50s 216ms/step - loss: 1.7902 - accuracy: 0.5408 - val_loss: 2.3627 - val_accuracy: 0.5161
Epoch 22/30
229/229 [==============================] - 52s 229ms/step - loss: 1.7189 - accuracy: 0.5556 - val_loss: 2.4280 - val_accuracy: 0.5210
Epoch 23/30
229/229 [==============================] - 52s 225ms/step - loss: 1.6688 - accuracy: 0.5602 - val_loss: 2.5961 - val_accuracy: 0.5090
Epoch 24/30
229/229 [==============================] - 52s 228ms/step - loss: 1.6106 - accuracy: 0.5765 - val_loss: 2.4716 - val_accuracy: 0.5336
Epoch 25/30
229/229 [==============================] - 53s 233ms/step - loss: 1.5767 - accuracy: 0.5803 - val_loss: 2.5784 - val_accuracy: 0.5347
Epoch 26/30
229/229 [==============================] - 53s 232ms/step - loss: 1.5229 - accuracy: 0.5887 - val_loss: 2.6759 - val_accuracy: 0.5418
Epoch 27/30
229/229 [==============================] - 54s 237ms/step - loss: 1.4854 - accuracy: 0.6074 - val_loss: 2.6941 - val_accuracy: 0.5298
Epoch 28/30
229/229 [==============================] - 50s 218ms/step - loss: 1.3859 - accuracy: 0.6271 - val_loss: 2.7268 - val_accuracy: 0.5402
Epoch 29/30
229/229 [==============================] - 51s 223ms/step - loss: 1.3545 - accuracy: 0.6328 - val_loss: 2.8625 - val_accuracy: 0.5200
Epoch 30/30
229/229 [==============================] - 50s 217ms/step - loss: 1.3333 - accuracy: 0.6383 - val_loss: 2.8598 - val_accuracy: 0.5030
 tiempo transcurrido (segundos) = 1544.9217319488525