C:\proyectos\proyectos09\SKYNNET\SW2\pruebas_masivas>py CNN_alexnet_caltech_adam_split02.py 1
**************************************
*     MASSIVE MLP                    *
*   programa de test                 *
*                                    *
* usage:                             *
* py massive_MLP.py <RF>             *
*    RF: reduction factor to N/RF    *
* input params:                      *
*    - RF 1.0
*                                    *
**************************************
2024-02-02 20:37:29.091757: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
2024-02-02 20:37:29.092096: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2024-02-02 20:37:29.093851: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
informative features
--------------------
  class names: ['accordion', 'airplanes', 'anchor', 'ant', 'background_google', 'barrel', 'bass', 'beaver', 'binocular', 'bonsai', 'brain', 'brontosaurus', 'buddha', 'butterfly', 'camera', 'cannon', 'car_side', 'ceiling_fan', 'cellphone', 'chair', 'chandelier', 'cougar_body', 'cougar_face', 'crab', 'crayfish', 'crocodile', 'crocodile_head', 'cup', 'dalmatian', 'dollar_bill', 'dolphin', 'dragonfly', 'electric_guitar', 'elephant', 'emu', 'euphonium', 'ewer', 'faces', 'faces_easy', 'ferry', 'flamingo', 'flamingo_head', 'garfield', 'gerenuk', 'gramophone', 'grand_piano', 'hawksbill', 'headphone', 'hedgehog', 'helicopter', 'ibis', 'inline_skate', 'joshua_tree', 'kangaroo', 'ketch', 'lamp', 'laptop', 'leopards', 'llama', 'lobster', 'lotus', 'mandolin', 'mayfly', 'menorah', 'metronome', 'minaret', 'motorbikes', 'nautilus', 'octopus', 'okapi', 'pagoda', 'panda', 'pigeon', 'pizza', 'platypus', 'pyramid', 'revolver', 'rhino', 'rooster', 'saxophone', 'schooner', 'scissors', 'scorpion', 'sea_horse', 'snoopy', 'soccer_ball', 'stapler', 'starfish', 'stegosaurus', 'stop_sign', 'strawberry', 'sunflower', 'tick', 'trilobite', 'umbrella', 'watch', 'water_lilly', 'wheelchair', 'wild_cat', 'windsor_chair', 'wrench', 'yin_yang']
  num clases: 102

datasets sizes. IF batch_size is used THEN this is the number of batches
-------------------------------------------------------------------------
  Train set size:  9144  batches of  1  elements

ds_train contents ( caltech101 )
--------------------------------
<PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>

dataset to numpy conversion ( caltech101 )
------------------------------------------
  convirtiendo DS en arrays numpy...( caltech101 )
C:\proyectos\proyectos09\SKYNNET\SW2\pruebas_masivas\CNN_alexnet_caltech_adam_split02.py:255: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  x_train = np.array(list(map(lambda x: x[0], ds_train_npy)))
  conversion OK
   x_train numpy shape: (9144,)
   y_train numpy shape: (9144,)


deconstruccion
----------------
 n_clases:  102    reduction to  102
 NUM MAQUINAS float= 1.0
 NUM MAQUINAS= 1

Filter data (deconstruction)
---------------------------
  number of examples:
   x_train: (9144,)
   y_train: (9144,)

Aadaptando imagenes...( caltech101 )
------------------------------------
ds_name= caltech101 --> h2= 227 w2= 227

channels:  3
 nuevo array: samples: 9144  h2= 227  w2= 227  channels2= 3

Normalizando... caltech101
-------------------------
array A --> dtype= float32
array train --> dtype= uint8
datos cargados en arrays
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv2d (Conv2D)             (None, 55, 55, 96)        34944

 max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0
 )

 conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656

 max_pooling2d_1 (MaxPooling  (None, 13, 13, 256)      0
 2D)

 conv2d_2 (Conv2D)           (None, 13, 13, 384)       885120

 conv2d_3 (Conv2D)           (None, 13, 13, 384)       1327488

 conv2d_4 (Conv2D)           (None, 13, 13, 256)       884992

 max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0
 2D)

 flatten (Flatten)           (None, 9216)              0

 dense (Dense)               (None, 4096)              37752832

 dropout (Dropout)           (None, 4096)              0

 dense_1 (Dense)             (None, 4096)              16781312

 dropout_1 (Dropout)         (None, 4096)              0

 dense_2 (Dense)             (None, 102)               417894

=================================================================
Total params: 58,699,238
Trainable params: 58,699,238
Non-trainable params: 0
_________________________________________________________________
None

DS: caltech101  n_clases:  102 factor= 1.0  reduction to  102  categories using  1  machines

OPTIMIZER= adam
---------------------------
OPTIMIZER= adam
epocas finales: 30 , split = 0.2 batch= 32  samples: 9144

Epoch 1/30
229/229 [==============================] - ETA: 0s - loss: 4.0438 - accuracy: 0.16492024-02-02 20:47:31.841678: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1130958492 exceeds 10% of free system memory.
229/229 [==============================] - 538s 2s/step - loss: 4.0438 - accuracy: 0.1649 - val_loss: 3.0567 - val_accuracy: 0.3565
Epoch 2/30
229/229 [==============================] - 517s 2s/step - loss: 3.5281 - accuracy: 0.2442 - val_loss: 2.9287 - val_accuracy: 0.3827
Epoch 3/30
229/229 [==============================] - 515s 2s/step - loss: 3.3612 - accuracy: 0.2643 - val_loss: 2.6892 - val_accuracy: 0.3969
Epoch 4/30
229/229 [==============================] - 541s 2s/step - loss: 3.1764 - accuracy: 0.2928 - val_loss: 2.4383 - val_accuracy: 0.4549
Epoch 5/30
229/229 [==============================] - 538s 2s/step - loss: 2.9624 - accuracy: 0.3266 - val_loss: 2.1572 - val_accuracy: 0.4975
Epoch 6/30
229/229 [==============================] - 530s 2s/step - loss: 2.7465 - accuracy: 0.3624 - val_loss: 1.9640 - val_accuracy: 0.5473
Epoch 7/30
229/229 [==============================] - 519s 2s/step - loss: 2.4598 - accuracy: 0.4161 - val_loss: 1.8719 - val_accuracy: 0.5604
Epoch 8/30
229/229 [==============================] - 518s 2s/step - loss: 2.1796 - accuracy: 0.4659 - val_loss: 1.7632 - val_accuracy: 0.5850
Epoch 9/30
229/229 [==============================] - 522s 2s/step - loss: 1.9762 - accuracy: 0.5154 - val_loss: 1.6136 - val_accuracy: 0.6140
Epoch 10/30
229/229 [==============================] - 519s 2s/step - loss: 1.7856 - accuracy: 0.5493 - val_loss: 1.6872 - val_accuracy: 0.6085
Epoch 11/30
229/229 [==============================] - 517s 2s/step - loss: 1.5673 - accuracy: 0.5899 - val_loss: 1.6598 - val_accuracy: 0.6091
Epoch 12/30
229/229 [==============================] - 542s 2s/step - loss: 1.3337 - accuracy: 0.6503 - val_loss: 1.6558 - val_accuracy: 0.6227
Epoch 13/30
229/229 [==============================] - 580s 3s/step - loss: 1.1738 - accuracy: 0.6845 - val_loss: 1.6370 - val_accuracy: 0.6408
Epoch 14/30
229/229 [==============================] - 578s 3s/step - loss: 0.9933 - accuracy: 0.7274 - val_loss: 1.7580 - val_accuracy: 0.6271
Epoch 15/30
229/229 [==============================] - 576s 3s/step - loss: 0.8825 - accuracy: 0.7546 - val_loss: 1.7771 - val_accuracy: 0.6304
Epoch 16/30
229/229 [==============================] - 535s 2s/step - loss: 0.7045 - accuracy: 0.8042 - val_loss: 1.9567 - val_accuracy: 0.6184
Epoch 17/30
229/229 [==============================] - 533s 2s/step - loss: 0.6203 - accuracy: 0.8228 - val_loss: 2.0880 - val_accuracy: 0.6200
Epoch 18/30
229/229 [==============================] - 532s 2s/step - loss: 0.5759 - accuracy: 0.8383 - val_loss: 1.9748 - val_accuracy: 0.6342
Epoch 19/30
229/229 [==============================] - 520s 2s/step - loss: 0.4903 - accuracy: 0.8626 - val_loss: 2.0978 - val_accuracy: 0.6282
Epoch 20/30
229/229 [==============================] - 512s 2s/step - loss: 0.4207 - accuracy: 0.8822 - val_loss: 2.1954 - val_accuracy: 0.6227
Epoch 21/30
229/229 [==============================] - 511s 2s/step - loss: 0.3890 - accuracy: 0.8893 - val_loss: 2.2101 - val_accuracy: 0.6364
Epoch 22/30
229/229 [==============================] - 509s 2s/step - loss: 0.4459 - accuracy: 0.8783 - val_loss: 2.1441 - val_accuracy: 0.6391
Epoch 23/30
229/229 [==============================] - 507s 2s/step - loss: 0.4099 - accuracy: 0.8891 - val_loss: 2.2015 - val_accuracy: 0.6162
Epoch 24/30
229/229 [==============================] - 542s 2s/step - loss: 0.3177 - accuracy: 0.9098 - val_loss: 2.2153 - val_accuracy: 0.6288
Epoch 25/30
229/229 [==============================] - 566s 2s/step - loss: 0.3681 - accuracy: 0.9028 - val_loss: 2.2120 - val_accuracy: 0.6364
Epoch 26/30
229/229 [==============================] - 520s 2s/step - loss: 0.3440 - accuracy: 0.9098 - val_loss: 2.3712 - val_accuracy: 0.6249
Epoch 27/30
229/229 [==============================] - 539s 2s/step - loss: 0.2816 - accuracy: 0.9224 - val_loss: 2.2751 - val_accuracy: 0.6260
Epoch 28/30
229/229 [==============================] - 523s 2s/step - loss: 0.2516 - accuracy: 0.9311 - val_loss: 2.3321 - val_accuracy: 0.6320
Epoch 29/30
229/229 [==============================] - 531s 2s/step - loss: 0.2383 - accuracy: 0.9348 - val_loss: 2.4805 - val_accuracy: 0.6195
Epoch 30/30
229/229 [==============================] - 521s 2s/step - loss: 0.2931 - accuracy: 0.9191 - val_loss: 2.3168 - val_accuracy: 0.6233
 tiempo transcurrido (segundos) = 15949.969113349915